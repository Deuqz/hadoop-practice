{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080d4c8c-0f73-44fa-ae0e-80c1782a654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting phrase.py\n"
     ]
    }
   ],
   "source": [
    "%%file phrase.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRPhrases(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_phrase,\n",
    "                   reducer=self.reducer_max_phrase),\n",
    "            MRStep(reducer=self.reducer_sort)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_phrase(self, _, line):\n",
    "        splitted = line.split('\" \"')\n",
    "        if len(splitted) < 3:\n",
    "            return ('', '')\n",
    "        _, character, phrase = splitted\n",
    "        yield (character, phrase[:-1])\n",
    "\n",
    "    def reducer_max_phrase(self, character, phrases):\n",
    "        yield None, (character, max(phrases, key=len))\n",
    "\n",
    "    def reducer_sort(self, _, characters_phrases):\n",
    "        sorted_phrases = sorted(characters_phrases, key=lambda x: -len(x[1]))\n",
    "        for character, phrase in sorted_phrases:\n",
    "            if character == '':\n",
    "                continue\n",
    "            yield (character, phrase)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRPhrases.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fe57cbc-0968-4cbb-a5b4-ed5074531757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/phrase.root.20231129.201304.590647\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/phrase.root.20231129.201304.590647/output\n",
      "Streaming final output from /tmp/phrase.root.20231129.201304.590647/output...\n",
      "\"LEIA\"\t\"General Kenobi, years ago you served my father in the Clone Wars.  Now he begs you to help him in his struggle against the Empire.  I regret that I am unable to present my father's request to you in person, but my ship has fallen under attack and I'm afraid my mission to bring you to Alderaan has failed.  I have placed information vital to the survival of the Rebellion into the memory systems of this R2 unit.  My father will know how to retrieve it.  You must see this droid safely delivered to him on Alderaan.  This is our most desperate hour.  Help me, Obi-Wan Kenobi, you're my only hope.\"\n",
      "\"BIGGS\"\t\"I feel for you, Luke, you're going to have to learn what seems to be important or what really is important.  What good is all your uncle's work if it's taken over by the Empire?...  You know they're starting to nationalize commerce in the central systems...it won't be long before your uncle is merely a tenant, slaving for the greater glory of the Empire.\"\n",
      "\"DODONNA\"\t\"The approach will not be easy.  You are required to maneuver straight down this trench and skim the surface to this point.  The target area is only two meters wide.  It's a small thermal exhaust port, right below the main port.  The shaft leads directly to the reactor system.  A precise hit will start a chain reaction which should destroy the station.\"\n",
      "\"JABBA\"\t\"Put your blasters away.  Han, my boy, I'm only doing this because you're the best and I need you.  So, for an extra, say... twenty percent I'll give you a little more time... but this is it.  If you disappoint me again, I'll put a price on your head so large you won't be able to go near a civilized system for the rest of your short life.\"\n",
      "\"LUKE\"\t\"... so I cut off my power, shut down the afterburners and came in low on Deak's trail.  I was so close I thought I was going to fry my instruments. As it was I busted up the Skyhopper pretty bad.  Uncle Owen was pretty upset.  He grounded me for the rest of the season.  You should have been there... it was fantastic.\"\n",
      "\"TARKIN\"\t\"Not after we demonstrate the power of this station.  In a way, you have determined the choice of the planet that'll be destroyed first.  Since you are reluctant to provide us with the location of the Rebel base, I have chosen to test this station's destructive power... on your home planet of Alderaan.\"\n",
      "\"THREEPIO\"\t\"He says he's the property of Obi-Wan Kenobi, a resident of these parts.  And it's a private message for him.  Quite frankly, sir, I don't know what he's talking about.  Our last master was Captain Antilles, but with what we've been through, this little R2 unit has become a bit eccentric.\"\n",
      "\"BEN\"\t\"A young Jedi named Darth Vader, who was a pupil of mine until he turned to evil, helped the Empire hunt down and destroy the Jedi Knights.  He betrayed and murdered your father.  Now the Jedi are all but extinct.  Vader was seduced by the dark side of the Force.\"\n",
      "\"VADER\"\t\"Don't play games with me, Your Highness.  You weren't on any mercy mission this time.  You passed directly through a restricted system.  Several transmissions were beamed to this ship by Rebel spies.  I want to know what happened to the plans they sent you.\"\n",
      "\"HAN\"\t\"Kid, I've flown from one side of this galaxy to the other.  I've seen a lot of strange stuff, but I've never seen anything to make me believe there's one all-powerful force controlling everything.  There's no mystical energy field that controls my destiny.\"\n",
      "\"MOTTI\"\t\"Don't try to frighten us with your sorcerer's ways, Lord Vader.  Your sad devotion to that ancient religion has not helped you conjure up the stolen data tapes, or given you clairvoyance enough to find the Rebel's hidden fort...\"\n",
      "\"OFFICER CASS\"\t\"Our scout ships have reached Dantooine.  They found the remains of a Rebel base, but they estimate that it has been deserted for some time.  They are now conducting an extensive search of the surrounding systems.\"\n",
      "\"GREEDO\"\t\"It's too late.  You should have paid him when you had the chance.  Jabba's put a price on your head, so large that every bounty hunter in the galaxy will be looking for you.  I'm lucky I found you first.\"\n",
      "\"OWEN\"\t\"Harvest is when I need you the most.  Only one more season.  This year we'll make enough on the harvest so I'll be able to hire some more hands.  And then you can go to the Academy next year.\"\n",
      "\"TAGGE\"\t\"And what of the Rebellion?  If the Rebels have obtained a complete technical readout of this station, it is possible, however unlikely, that they might find a weakness and exploit it.\"\n",
      "\"SECOND OFFICER\"\t\"Lord Vader, the battle station plans are not aboard this ship!  And no transmissions were made.  An escape pod was jettisoned during the fighting, but no life forms were aboard.\"\n",
      "\"FIXER\"\t\"I keep telling you, the Rebellion is a long way from here.  I doubt if the Empire would even fight to keep this system.  Believe me Luke, this planet is a big hunk of nothing...\"\n",
      "\"OFFICER\"\t\"There's no one on board, sir.  According to the log, the crew abandoned ship right after takeoff.  It must be a decoy, sir.  Several of the escape pods have been jettisoned.\"\n",
      "\"RED LEADER\"\t\"I met your father once when I was just a boy.  He was a great pilot.  You'll do all right.  If you've got half of your father's skill, you'll do better than all right.\"\n",
      "\"VOICE\"\t\"We've captured a freighter entering the remains of the Alderaan system.  It's markings match those of a ship that blasted its way out of Mos Eisley.\"\n",
      "\"DEATH STAR INTERCOM VOICE\"\t\"We are approaching the planet Yavin.  The Rebel base is on a moon on the far side.  We are preparing to orbit the planet.\"\n",
      "\"COMMANDER\"\t\"Holding her is dangerous.  If word of this gets out, it could generate sympathy for the Rebellion in the senate.\"\n",
      "\"HUMAN\"\t\"Don't insult us.  You just watch yourself.  We're wanted men.  I have the death sentence on twelve systems.\"\n",
      "\"TROOPER\"\t\"The ship's all yours.  If the scanners pick up anything, report it immediately.  All right, let's go.\"\n",
      "\"WEDGE\"\t\"My scope shows the tower, but I can't see the exhaust port!  Are you sure the computer can hit it?\"\n",
      "\"ASTRO-OFFICER\"\t\"We count thirty Rebel ships, Lord Vader.  But they're so small they're evading our turbo-lasers!\"\n",
      "\"REBEL OFFICER\"\t\"We intercepted no transmissions. Aaah...  This is a consular ship. Were on a diplomatic mission.\"\n",
      "\"AUNT BERU\"\t\"Owen, he can't stay here forever.  Most of his friends have gone.  It means so much to him.\"\n",
      "\"MASSASSI INTERCOM VOICE\"\t\"Stand-by alert.  Death Star approaching.  Estimated time to firing range, fifteen minutes.\"\n",
      "\"WILLARD\"\t\"When we heard about Alderaan, we were afraid that you were... lost along with your father.\"\n",
      "\"IMPERIAL OFFICER\"\t\"The final check-out is complete.  All systems are operational.  What course shall we set?\"\n",
      "\"CONTROL OFFICER\"\t\"Squad leaders, we've picked up a new group of signals.  Enemy fighters coming your way.\"\n",
      "\"GOLD LEADER\"\t\"Pardon me for asking, sir, but what good are snub fighters going to be against that?\"\n",
      "\"BASE VOICE\"\t\"His computer's off.  Luke, you switched off your targeting computer.  What's wrong?\"\n",
      "\"INTERCOM VOICE\"\t\"Governor Tarkin, we have an emergency alert in detention block AA-twenty-three.\"\n",
      "\"GANTRY OFFICER\"\t\"TX-four-one-two.  Why aren't you at your post?  TX-four-one-two, do you copy? \"\n",
      "\"CAPTAIN\"\t\"Hold your fire.  There are no life forms.  It must have been short-circuited.\"\n",
      "\"MAN\"\t\"All flight troops, man your stations.  All flight troops, man your stations.\"\n",
      "\"BERU\"\t\"Luke, tell Owen that if he gets a translator to be sure it speaks Bocce.\"\n",
      "\"BARTENDER\"\t\"Your droids. They'll have to wait outside.  We don't want them here.\"\n",
      "\"GOLD FIVE\"\t\"I'd say about twenty guns.  Some on the surface, some on the towers.\"\n",
      "\"CHIEF\"\t\"This R2 unit of your seems a bit beat up.  Do you want a new one?\"\n",
      "\"VOICE OVER DEATH STAR INTERCOM\"\t\"Clear Bay twenty-three-seven.  We are opening the magnetic field.\"\n",
      "\"RED TEN\"\t\"There's a heavy fire zone on this side. Red Five, where are you?\"\n",
      "\"FIRST TROOPER\"\t\"Someone was in the pod.  The tracks go off in this direction. \"\n",
      "\"CAMIE\"\t\"It was just Wormie on another rampage.\"\n",
      "\"GOLD TWO\"\t\"Computer's locked.  Getting a signal.\"\n",
      "\"TECHNICIAN\"\t\"We'll get to work on him right away.\"\n",
      "\"WOMAN\"\t\"I've told you kids to slow down!\"\n",
      "\"CREATURE\"\t\"Negola dewaghi wooldugger?!?\"\n",
      "\"FIRST OFFICER\"\t\"Follow me!  You stand guard.\"\n",
      "\"SECOND TROOPER\"\t\"Maybe it's another drill.\"\n",
      "\"CHIEF PILOT\"\t\"There goes another one.\"\n",
      "\"RED ELEVEN\"\t\"Red Eleven standing by.\"\n",
      "\"RED SEVEN\"\t\"Red Seven standing by.\"\n",
      "\"DEAK\"\t\"Not again!  Forget it.\"\n",
      "\"RED NINE\"\t\"Red Nine standing by.\"\n",
      "\"PORKINS\"\t\"Red Six standing by.\"\n",
      "\"TROOPER VOICE\"\t\"Open up in there!\"\n",
      "\"WINGMAN\"\t\"Yes, sir.\"\n",
      "Removing temp directory /tmp/phrase.root.20231129.201304.590647...\n"
     ]
    }
   ],
   "source": [
    "!python3 phrase.py ../SW_EpisodeIV.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d2fac2e-4611-4862-8ed4-7265c8553031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CLASSPATH'] = '/opt/hadoop/etc/hadoop:/opt/hadoop/share/hadoop/common/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/common/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jul-to-slf4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/common/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/common/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/common/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/common/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/common/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/common/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/common/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/common/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/common/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/common/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/common/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/slf4j-api-1.7.36.jar:/opt/hadoop/share/hadoop/common/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/common/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/common/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/common/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/common/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/common/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/common/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/common/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/common/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/common/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/common/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/common/hadoop-registry-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-kms-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6-tests.jar:/opt/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-recipes-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-buffer-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/zookeeper-jute-3.6.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-rxtx-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-all-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/snappy-java-1.1.8.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/opt/hadoop/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/metrics-core-3.2.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-proxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-memcache-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/opt/hadoop/share/hadoop/hdfs/lib/jakarta.activation-api-1.2.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/gson-2.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-auth-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/opt/hadoop/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-codec-1.15.jar:/opt/hadoop/share/hadoop/hdfs/lib/jettison-1.5.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-json-1.20.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-xml-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-annotations-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-native-macos-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-sctp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-dns-classes-macos-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-udt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/re2j-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-text-1.10.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-smtp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-unix-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-kqueue-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-net-3.9.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/avro-1.7.7.jar:/opt/hadoop/share/hadoop/hdfs/lib/kotlin-stdlib-common-1.4.10.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-core-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-configuration2-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/paranamer-2.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-resolver-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-client-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-socks-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-security-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/okhttp-4.9.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-util-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-protobuf_3_7-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/HikariCP-java7-2.4.12.jar:/opt/hadoop/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-dns-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-epoll-4.1.89.Final-linux-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/curator-framework-5.2.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-aarch_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/hadoop-shaded-guava-1.1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-http-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-haproxy-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-handler-ssl-ocsp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-servlet-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jersey-server-1.19.4.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-http2-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-classes-epoll-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-io-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/jetty-webapp-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/opt/hadoop/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-xml-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-mqtt-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/reload4j-1.2.22.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/woodstox-core-5.4.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-stomp-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-common-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/jackson-databind-2.12.7.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-lang3-3.12.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-codec-redis-4.1.89.Final.jar:/opt/hadoop/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/opt/hadoop/share/hadoop/hdfs/lib/netty-transport-native-kqueue-4.1.89.Final-osx-x86_64.jar:/opt/hadoop/share/hadoop/hdfs/lib/okio-2.8.0.jar:/opt/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/opt/hadoop/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/opt/hadoop/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-client-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6-tests.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-native-client-3.3.6.jar:/opt/hadoop/share/hadoop/hdfs/hadoop-hdfs-rbf-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.3.6-tests.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.3.6.jar:/opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.3.6.jar:/opt/hadoop/share/hadoop/yarn:/opt/hadoop/share/hadoop/yarn/lib/jetty-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-annotations-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-common-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/objenesis-2.6.jar:/opt/hadoop/share/hadoop/yarn/lib/jline-3.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-jndi-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-client-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/opt/hadoop/share/hadoop/yarn/lib/java-util-1.9.0.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-server-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-base-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/opt/hadoop/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-client-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/opt/hadoop/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.12.7.jar:/opt/hadoop/share/hadoop/yarn/lib/javax-websocket-client-impl-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcprov-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-servlet-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/jna-5.2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jersey-guice-1.19.4.jar:/opt/hadoop/share/hadoop/yarn/lib/json-io-2.5.1.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-commons-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-api-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/opt/hadoop/share/hadoop/yarn/lib/jetty-plus-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/opt/hadoop/share/hadoop/yarn/lib/bcpkix-jdk15on-1.68.jar:/opt/hadoop/share/hadoop/yarn/lib/websocket-server-9.4.51.v20230217.jar:/opt/hadoop/share/hadoop/yarn/lib/snakeyaml-2.0.jar:/opt/hadoop/share/hadoop/yarn/lib/asm-tree-9.4.jar:/opt/hadoop/share/hadoop/yarn/lib/fst-2.50.jar:/opt/hadoop/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/opt/hadoop/share/hadoop/yarn/lib/javax.websocket-client-api-1.0.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-registry-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-api-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-api-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-router-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-mawo-core-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-client-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.3.6.jar:/opt/hadoop/share/hadoop/yarn/hadoop-yarn-services-core-3.3.6.jar'\n",
    "from pyarrow import fs\n",
    "hdfs = fs.HadoopFileSystem(\"namenode\", 8020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72377acc-1049-4098-a79f-72000487a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs.create_dir(\"/pyarrow/task2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d088157c-4cb1-4562-9932-fba58b6e9ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/phrase.root.20231129.201526.838747\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201526.838747/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201526.838747/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8232481687655127694/] [] /tmp/streamjob4435127196564520580.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701282605617_0011\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701282605617_0011\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701282605617_0011\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701282605617_0011/\n",
      "  Running job: job_1701282605617_0011\n",
      "  Job job_1701282605617_0011 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701282605617_0011 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201526.838747/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=82374\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=9064\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=74563\n",
      "\t\tFILE: Number of bytes written=997264\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=82568\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=9064\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5337088\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2341888\n",
      "\t\tTotal time spent by all map tasks (ms)=5212\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10424\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2287\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4574\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5212\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2287\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2350\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=126\n",
      "\t\tInput split bytes=194\n",
      "\t\tMap input records=1011\n",
      "\t\tMap output bytes=72416\n",
      "\t\tMap output materialized bytes=74569\n",
      "\t\tMap output records=1010\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=312967168\n",
      "\t\tPeak Map Virtual memory (bytes)=2593075200\n",
      "\t\tPeak Reduce Physical memory (bytes)=261857280\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2599198720\n",
      "\t\tPhysical memory (bytes) snapshot=883187712\n",
      "\t\tReduce input groups=60\n",
      "\t\tReduce input records=1010\n",
      "\t\tReduce output records=60\n",
      "\t\tReduce shuffle bytes=74569\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=2020\n",
      "\t\tTotal committed heap usage (bytes)=925892608\n",
      "\t\tVirtual memory (bytes) snapshot=7784861696\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7683158447772356604/] [] /tmp/streamjob7262515694473029633.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701282605617_0012\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701282605617_0012\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701282605617_0012\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701282605617_0012/\n",
      "  Running job: job_1701282605617_0012\n",
      "  Job job_1701282605617_0012 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701282605617_0012 completed successfully\n",
      "  Output directory: hdfs:///pyarrow/task2/resIV\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=13160\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8584\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=9254\n",
      "\t\tFILE: Number of bytes written=866373\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=13460\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=8584\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4348928\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2252800\n",
      "\t\tTotal time spent by all map tasks (ms)=4247\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8494\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2200\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4400\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4247\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2200\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1650\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=128\n",
      "\t\tInput split bytes=300\n",
      "\t\tMap input records=60\n",
      "\t\tMap output bytes=9096\n",
      "\t\tMap output materialized bytes=9260\n",
      "\t\tMap output records=60\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=310788096\n",
      "\t\tPeak Map Virtual memory (bytes)=2593124352\n",
      "\t\tPeak Reduce Physical memory (bytes)=228134912\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2601181184\n",
      "\t\tPhysical memory (bytes) snapshot=808095744\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=60\n",
      "\t\tReduce output records=60\n",
      "\t\tReduce shuffle bytes=9260\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=120\n",
      "\t\tTotal committed heap usage (bytes)=857735168\n",
      "\t\tVirtual memory (bytes) snapshot=7787134976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///pyarrow/task2/resIV\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201526.838747...\n",
      "Removing temp directory /tmp/phrase.root.20231129.201526.838747...\n"
     ]
    }
   ],
   "source": [
    "!python3 phrase.py -r hadoop hdfs://namenode:8020/pyarrow/SW_EpisodeIV.txt --output /pyarrow/task2/resIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c80890a6-063e-4176-abed-9fa10b03f9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/phrase.root.20231129.201648.920685\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201648.920685/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201648.920685/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar3389846551015610300/] [] /tmp/streamjob8430043093858932894.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701282605617_0013\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701282605617_0013\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701282605617_0013\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701282605617_0013/\n",
      "  Running job: job_1701282605617_0013\n",
      "  Job job_1701282605617_0013 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701282605617_0013 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201648.920685/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=59583\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5573\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=52333\n",
      "\t\tFILE: Number of bytes written=952801\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=59775\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=5573\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=5012480\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2343936\n",
      "\t\tTotal time spent by all map tasks (ms)=4895\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9790\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2289\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4578\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4895\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2289\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2030\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=135\n",
      "\t\tInput split bytes=192\n",
      "\t\tMap input records=840\n",
      "\t\tMap output bytes=50594\n",
      "\t\tMap output materialized bytes=52339\n",
      "\t\tMap output records=839\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=312201216\n",
      "\t\tPeak Map Virtual memory (bytes)=2593112064\n",
      "\t\tPeak Reduce Physical memory (bytes)=264462336\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2598600704\n",
      "\t\tPhysical memory (bytes) snapshot=885501952\n",
      "\t\tReduce input groups=49\n",
      "\t\tReduce input records=839\n",
      "\t\tReduce output records=49\n",
      "\t\tReduce shuffle bytes=52339\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1678\n",
      "\t\tTotal committed heap usage (bytes)=929562624\n",
      "\t\tVirtual memory (bytes) snapshot=7783784448\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar9103444137868787197/] [] /tmp/streamjob1368014776501623745.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701282605617_0014\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701282605617_0014\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701282605617_0014\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701282605617_0014/\n",
      "  Running job: job_1701282605617_0014\n",
      "  Job job_1701282605617_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701282605617_0014 completed successfully\n",
      "  Output directory: hdfs:///pyarrow/task2/resV\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=8360\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5181\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=5712\n",
      "\t\tFILE: Number of bytes written=859286\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=8660\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=5181\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4383744\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2223104\n",
      "\t\tTotal time spent by all map tasks (ms)=4281\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8562\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2171\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4342\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4281\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2171\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1480\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=115\n",
      "\t\tInput split bytes=300\n",
      "\t\tMap input records=49\n",
      "\t\tMap output bytes=5590\n",
      "\t\tMap output materialized bytes=5718\n",
      "\t\tMap output records=49\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=310849536\n",
      "\t\tPeak Map Virtual memory (bytes)=2594910208\n",
      "\t\tPeak Reduce Physical memory (bytes)=224063488\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2600534016\n",
      "\t\tPhysical memory (bytes) snapshot=845053952\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=49\n",
      "\t\tReduce output records=49\n",
      "\t\tReduce shuffle bytes=5718\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=98\n",
      "\t\tTotal committed heap usage (bytes)=835715072\n",
      "\t\tVirtual memory (bytes) snapshot=7787917312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///pyarrow/task2/resV\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201648.920685...\n",
      "Removing temp directory /tmp/phrase.root.20231129.201648.920685...\n"
     ]
    }
   ],
   "source": [
    "!python3 phrase.py -r hadoop hdfs://namenode:8020/pyarrow/SW_EpisodeV.txt --output /pyarrow/task2/resV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "705bf8b7-0541-4c3e-a440-95f58bf61308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/phrase.root.20231129.201745.168169\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201745.168169/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201745.168169/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8385396393964415511/] [] /tmp/streamjob1344384055286040442.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701282605617_0015\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701282605617_0015\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701282605617_0015\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701282605617_0015/\n",
      "  Running job: job_1701282605617_0015\n",
      "  Job job_1701282605617_0015 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701282605617_0015 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201745.168169/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=52272\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6991\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=45706\n",
      "\t\tFILE: Number of bytes written=939550\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=52466\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=6991\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4971520\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2291712\n",
      "\t\tTotal time spent by all map tasks (ms)=4855\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9710\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2238\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4476\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4855\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2238\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1840\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=124\n",
      "\t\tInput split bytes=194\n",
      "\t\tMap input records=675\n",
      "\t\tMap output bytes=44288\n",
      "\t\tMap output materialized bytes=45712\n",
      "\t\tMap output records=674\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=311832576\n",
      "\t\tPeak Map Virtual memory (bytes)=2594197504\n",
      "\t\tPeak Reduce Physical memory (bytes)=232468480\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2599010304\n",
      "\t\tPhysical memory (bytes) snapshot=851546112\n",
      "\t\tReduce input groups=53\n",
      "\t\tReduce input records=674\n",
      "\t\tReduce output records=53\n",
      "\t\tReduce shuffle bytes=45712\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=1348\n",
      "\t\tTotal committed heap usage (bytes)=861929472\n",
      "\t\tVirtual memory (bytes) snapshot=7787220992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4643211100026284458/] [] /tmp/streamjob4560432211958690452.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701282605617_0016\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701282605617_0016\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701282605617_0016\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701282605617_0016/\n",
      "  Running job: job_1701282605617_0016\n",
      "  Job job_1701282605617_0016 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701282605617_0016 completed successfully\n",
      "  Output directory: hdfs:///pyarrow/task2/resVI\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=10487\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=6567\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=7147\n",
      "\t\tFILE: Number of bytes written=862159\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=10787\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=6567\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4530176\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2318336\n",
      "\t\tTotal time spent by all map tasks (ms)=4424\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8848\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2264\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4528\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4424\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2264\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1680\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=142\n",
      "\t\tInput split bytes=300\n",
      "\t\tMap input records=53\n",
      "\t\tMap output bytes=7013\n",
      "\t\tMap output materialized bytes=7153\n",
      "\t\tMap output records=53\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=310177792\n",
      "\t\tPeak Map Virtual memory (bytes)=2594566144\n",
      "\t\tPeak Reduce Physical memory (bytes)=227856384\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2598531072\n",
      "\t\tPhysical memory (bytes) snapshot=847360000\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=53\n",
      "\t\tReduce output records=53\n",
      "\t\tReduce shuffle bytes=7153\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=106\n",
      "\t\tTotal committed heap usage (bytes)=862453760\n",
      "\t\tVirtual memory (bytes) snapshot=7785144320\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///pyarrow/task2/resVI\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201745.168169...\n",
      "Removing temp directory /tmp/phrase.root.20231129.201745.168169...\n"
     ]
    }
   ],
   "source": [
    "!python3 phrase.py -r hadoop hdfs://namenode:8020/pyarrow/SW_EpisodeVI.txt --output /pyarrow/task2/resVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ac246a1-884c-441a-9932-cc9e85bed257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop/bin...\n",
      "Found hadoop binary: /opt/hadoop/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop...\n",
      "Found Hadoop streaming jar: /opt/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/phrase.root.20231129.201840.341486\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201840.341486/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201840.341486/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6965895134596396549/] [] /tmp/streamjob3464344134134301647.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701282605617_0017\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701282605617_0017\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701282605617_0017\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701282605617_0017/\n",
      "  Running job: job_1701282605617_0017\n",
      "  Job job_1701282605617_0017 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701282605617_0017 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201840.341486/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=185991\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=17103\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=172590\n",
      "\t\tFILE: Number of bytes written=1193321\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=186187\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=17103\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4914176\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2291712\n",
      "\t\tTotal time spent by all map tasks (ms)=4799\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=9598\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2238\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4476\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4799\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2238\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2110\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=127\n",
      "\t\tInput split bytes=196\n",
      "\t\tMap input records=2524\n",
      "\t\tMap output bytes=167298\n",
      "\t\tMap output materialized bytes=172596\n",
      "\t\tMap output records=2523\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=310091776\n",
      "\t\tPeak Map Virtual memory (bytes)=2594664448\n",
      "\t\tPeak Reduce Physical memory (bytes)=262594560\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2598739968\n",
      "\t\tPhysical memory (bytes) snapshot=882229248\n",
      "\t\tReduce input groups=129\n",
      "\t\tReduce input records=2523\n",
      "\t\tReduce output records=129\n",
      "\t\tReduce shuffle bytes=172596\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=5046\n",
      "\t\tTotal committed heap usage (bytes)=930611200\n",
      "\t\tVirtual memory (bytes) snapshot=7787765760\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar4242029948265552727/] [] /tmp/streamjob5949274631359811785.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Connecting to ResourceManager at resourcemanager/172.18.0.6:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1701282605617_0018\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1701282605617_0018\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1701282605617_0018\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1701282605617_0018/\n",
      "  Running job: job_1701282605617_0018\n",
      "  Job job_1701282605617_0018 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1701282605617_0018 completed successfully\n",
      "  Output directory: hdfs:///pyarrow/task2/resALL\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21199\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=16071\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=17469\n",
      "\t\tFILE: Number of bytes written=882806\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21499\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=16071\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=4596736\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2245632\n",
      "\t\tTotal time spent by all map tasks (ms)=4489\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8978\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2193\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=4386\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=4489\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2193\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1700\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=133\n",
      "\t\tInput split bytes=300\n",
      "\t\tMap input records=129\n",
      "\t\tMap output bytes=17154\n",
      "\t\tMap output materialized bytes=17475\n",
      "\t\tMap output records=129\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=310820864\n",
      "\t\tPeak Map Virtual memory (bytes)=2594807808\n",
      "\t\tPeak Reduce Physical memory (bytes)=225775616\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2599067648\n",
      "\t\tPhysical memory (bytes) snapshot=844259328\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=129\n",
      "\t\tReduce output records=129\n",
      "\t\tReduce shuffle bytes=17475\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=258\n",
      "\t\tTotal committed heap usage (bytes)=870318080\n",
      "\t\tVirtual memory (bytes) snapshot=7784603648\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///pyarrow/task2/resALL\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/phrase.root.20231129.201840.341486...\n",
      "Removing temp directory /tmp/phrase.root.20231129.201840.341486...\n"
     ]
    }
   ],
   "source": [
    "!python3 phrase.py -r hadoop hdfs://namenode:8020/pyarrow/SW_EpisodeALL.txt --output /pyarrow/task2/resALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a73c82df-73e7-4c06-a962-7cd283c8e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ['resIV', 'resV', 'resVI', 'resALL']\n",
    "\n",
    "for res in results:\n",
    "    with hdfs.open_input_stream(f\"/pyarrow/task2/{res}/part-00000\") as file:\n",
    "        data = file.readall().decode()\n",
    "    with open(f'data_{res}.txt', 'w') as f:\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b262e6e-fa9d-4e30-ba94-b53a1d23f36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
